% !TeX root = ../main.tex

\chapter{相关模型介绍}

\section{pix2pixHD的模型结构}

\subsection{简介}

pix2pixHD是一种基于条件生成对抗网络的图像翻译模型，能够从输入的语义标签图出发生成高质量、高分辨率的图像。

在它之前的方法存在两个问题：1.很难生成高分辨率的图像，如pix2pix\cite{pix2pix}；2.生成的图像缺少细节特征和真实的纹理，如CRN\cite{crn}。
  
所以，为了解决以上两个问题，该模型提出了一个新的鲁棒性更强的对抗学习损失函数，并采用了新的多尺度生成器和判别器结构。运用该模型可以生成分辨率达到$2048×1024$的真实感更强的图像，效果超越了以前的方法。

此外，该模型还可以对生成图像进行交互的图像编辑。首先，可以在输入中加入对象的实例分割信息，实现对物体的编辑，比如在生成图像中增删物体或者改变物体的类别；其次，给定相同的输入，可以编辑生成图像中某一物体的外观。

\subsection{网络结构}

\subsubsection{多层级的生成器}

生成器网络分为两个子网络：$G_1$和$G_2$，如图~\ref{fig:pix2pixHD}所示。$G_1$称为全局生成器网络(global generator network)，$G_2$称为局部增强器网络(local enhancer network)。$G_1$用来生成基础图像，而$G_2$用来提高图像的分辨率。为了进一步提高分辨率，甚至可以继续叠加$G_2$。

$G_1$由3部分组成，分别是一个卷积前端$G_1^{(F)}$，一系列的残差模块$G_1^{(R)}$和一个逆卷积后端$G_1^{(B)}$。$G_1^{(F)}$由5个卷积层组成，$G_1^{(R)}$由9个残差模块组成，$G_1^{(B)}$由4个逆卷积层和最后的1个卷积层组成。这种结构被证明用在神经风格迁移任务中很成功。网络中每一个卷积和逆卷积操作后都用实例标准化进行处理，使训练过程更加鲁棒。

$G_2$ 也是由3部分组成：一个卷积前端$G_2^{(F)}$，一系列的残差模块$G_2^{(R)}$和一个逆卷积后端$G_2^{(B)}$。其中，$G_2^{(F)}$由2个卷积层组成，$G_2^{(R)}$由3个残差模块组成，$G_2^{(B)}$由1个逆卷积层和1个卷积层组成。$G_2^{(F)}$输出的特征图与$G_1^{(B)}$第4个逆卷积层的特征图进行每像素相加，相加的结果作为$G_2^{(R)}$的输入。

在训练阶段，先分别训练$G_1$和$G_2$，然后将两部分合并训练，对网络参数进行微调。这种由粗到精的网络结构可以有效地融合局部和全局的信息，最终生成高分辨率、高真实感的图像。
\begin{figure}[htb]
	\centering
	\includegraphics[width=1 \textwidth]{pix2pixhd.png}
	\caption{pix2pixHD生成器网络结构}
	\label{fig:pix2pixHD}
\end{figure}


\subsubsection{多尺度的判别器}

为了判别高分辨率的真实图像与生成图像，判别器需要有大的感知野。如果通过增加网络深度或者提高卷积核尺寸来解决，将占用大量的显存，并且容易导致过拟合的问题。

所以该模型采用多尺度的判别器，由粗到精地分辨生成图像和真实图像。具体来说，使用了3个网络结构完全相同的判别器，分别称为$D_1$，$D_2$和$D_3$。将用于训练的真实图像和生成图像进行$1/2$和$1/4$降采样，连同原始尺寸图像，分别作为$D_1$，$D_2$和$D_3$的输入。对于以最小尺寸图像为输入的$D_2$来说，其拥有最大的感知野，能更好的对全局信息进行判别，因而可以使生成图像在全局视野下更真实；对于以原始尺寸图像为输入的$D_3$来说，其拥有最精细的粒度，对局部信息更敏感，可以促进细节的生成。

判别器的网络结构比较简单，由5个降采样的卷积层堆叠而成，其卷积核大小为4。

\subsection{目标函数}

该模型采用了3个目标函数，第一个是常规的对抗生成损失$L_{GAN}(G,D_k)$，第二个是特征匹配损失$L_{FM}(G,D_k)$，第三个是VGG损失$L_{VGG}(G)$，下面分别对这3个目标函数进行介绍。

\subsubsection{对抗生成损失}

$L_{GAN}(G,D_k)$是普通的对抗生成损失，没有需要特别说明之处。其数学表达如下所示：
\begin{equation}
	L_{GAN}(G,D_k) = \mathbb{E}_{(\vs,\vx)}[\log D_k(\vs,\vx)]+\mathbb{E}_\vs[\log (1-D(\vs,G(\vs)))]
\end{equation}

其中，$\vs$表示语义标签图的域，$\vx$表示对应的真实图像的域，$\mathbb{E}_\vs$表示$\mathbb{E}_{\vs \sim p_{data}(\vs)}$，$\mathbb{E}_{(\vs,\vx)}$表示$\mathbb{E}_{(\vs,\vx) \sim p_{data}(\vs,\vx)}$。

\subsubsection{特征匹配损失}

首先用多尺度判别器提取生成图像和真实图像的特征，然后对两类特征求$L_1$损失，使特征尽可能相似。这种匹配中间过程表达的做法可以帮助稳定训练过程。
其数学形式为：
\begin{equation}
	L_{FM}(G,D_k)=\mathbb{E}_{(\vs,\vx)} \sum_{i=1}^T{\frac{1}{N_{ki}} [\lVert D_{k}^{(i)}(\vs,\vx)-D_{k}^{(i)}(\vs,G(\vs)) \rVert_1]}
\end{equation}

$D_{k}^{(i)}$表示判别器$D_k$的第$i$层特征提取器，$T$表示判别器中特征提取的层数，$N_{ki}$表示第$k$个判别器第$i$层特征图的元素总数。

\subsubsection{VGG损失}

首先用预训练好的的VGG19模型作为特征提取器，将生成图像和真实图像分别输入$VGG19$，得到中间层的特征，然后对两类特征求$L_1$损失。其本质也是一种感知损失，按如下方式计算：
\begin{equation}
	L_{VGG}(G)=\mathbb{E}_{(\vs,\vx)} \sum_{i=1}^L{\frac{1}{M_{i}} [W_i \cdot \lVert V^{(i)}(\vx)-V^{(i)}(G(\vs)) \rVert_2]}
\end{equation}

其中，$V^{(i)}$表示VGG19中的第$i$层特征提取器，$W_i$为每层的权重参数，$M_i$代表每层特征图的元素总和，$L$为特征提取器的层数。
~\\

pix2pixHD总的目标函数如下所示：
\begin{equation}
	\underset{G}{\min}((\underset{D_1,D_2,D_3}{\max} \sum_{k=1,2,3}{L_{GAN}(G,D_k)})+\lambda (\frac{1}{3}\sum_{k=1,2,3}{L_{FM}(G,D_k) }+L_{VGG}{(G(\vs))}))
\end{equation}

\section{实例标准化}

实例标准化(IN)\cite{instance_norm}是众多标准化方法中的一种，由Ulyanov等人在2017年提出，在风格迁移、图像翻译等任务上有很好的作用效果。
实例标准化通过计算单样本单通道的均值和方差对输入进行标准化，首先对输入的单张特征图的某一通道的像素值求均值和方差，像素值减去均值后再除以方差进行标准化，之后再进行放缩和平移。其实现过程的数学表达形式如公式(2.5)$\sim$(2.8)所示：
\begin{equation}
	\mu_{ni} = \frac{1}{HW} \sum_{l=1}^{W}{\sum_{m=1}^{H}{x_{nilm}}} 
\end{equation}
\begin{equation}
	\sigma_{ni}^{2} = \frac{1}{HW} \sum_{l=1}^{W}{\sum_{m=1}^{H}{(x_{nilm}-\mu_{ni})^2}}
\end{equation}
\begin{equation}
	\hat{x}_{nijk} = \frac{x_{nijk}-\mu_{ni}}{\sqrt{\sigma_{ni}^{2}+\epsilon}}
\end{equation}
\begin{equation}
	y_{nijk} = \gamma \hat{x}_{nijk}+\beta
\end{equation}

用$\symbf{x} \in \mathbb{R}^{N \times C \times W \times H}$表示一个批次的图片或特征，$x_{nijk} \in \symbf{x}$表示其中的一个元素，其中，$n$表示在批次中的索引，$i$表示特征通道，$j,k$表示空间位置。$\gamma$和$\beta$是可学习的参数，分别对标准化后的值进行放缩和平移。$\epsilon$是为了防止方差为0而加入的微小的正数。

实例标准化有以下几个作用：

首先，对特征进行标准化，使其服从均值为0、方差为1的正态分布，缩小了同一通道内不同元素间的差异，从而避免了梯度消失和梯度爆炸的问题，使训练过程更加稳定。

其次，通过减少梯度对初始值尺度的依赖，从而可以用较大的学习率训练网络，从而加速网络的收敛。

其实，实例标准化与批标准化非常相似，唯一的不同是批标准化对一个批次内所有的特征图求均值和方差，而实例标准化只对单个样本进行计算。对于图像翻译这类任务，重点关注每张图像的内容，可以把每个样本都看成一个单独的域，因此批归一化便不再适用。因为批归一化考虑批次内所有样本的信息，从而造成每个样本细节信息的丢失。而实例归一化只考虑单样本单通道的信息，更适合对每像素有更高要求的任务。

实例归一化沿用了批归一化的$\gamma$和$\beta$两个参数，可以自适应地调节输出的范围，使其不致受限于标准高斯分布，提高了网络的表达能力。

\section{模型引入}

我们对pix2pixHD的模型做了一些改进，在其全局生成器$G_1$的基础上，去掉了前两层的实例标准化操作，作为我们自己的生成器。生成器的结构如图~\ref{fig:ours}所示:
\begin{figure}[htb]
	\centering
	\includegraphics[width=1 \textwidth]{ours.png}
	\caption{我们的模型的生成器网络结构}
	\label{fig:ours}
\end{figure}

判别器沿用了多级判别器的设计，并利用了PatchGAN\cite{pix2pix}的思路，最终判别器输出的是一个矩阵而非一个标量。若用$D_k,k=1,2,3$表示我们的判别器，其中$D_k$的网络结构如图~\ref{fig:ours_D}所示：
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6 \textwidth]{ours_D.png}
	\caption{我们模型的判别器$D_k$的网络结构}
	\label{fig:ours_D}
\end{figure}

我们把草图和人脸照片在通道方向连接起来作为判别器的输入，判别器的输出用来计算生成对抗损失$L_{GAN}$和特征匹配损失$L_{FM}$。我们的目标函数跟pix2pixHD中保持了一致，也是由$L_{GAN}$、$L_{FM}$和$L_{VGG}$ 3部分组成。

\section{数据集生成}

\subsection{CelebA-HQ数据集}

CelebA数据集是香港中文大学的开放数据库，搜集了10177位名人的202599张人脸照片，并且对每张图片标注了人脸特征点(landmark)。而CelebA-HQ数据集是在CelebA的基础上，利用PGGAN\cite{pggan}生成的一个新的数据集。

CelebA-HQ包含30000张分辨率为1024×1024人脸照片，因为裁剪图片时以人脸特征点为参照，所以每张图片都是全局对齐的，即有一个标准化的位置和角度。

\subsection{轮廓数据集}

我们从CelebA-HQ数据集的人脸照片中提取了68个特征点，然后用像素值为2的线将这些点依次连接，就形成了轮廓图。

为了实验需要，我们将图片进行了缩放，每张图的分辨率为512×512. 经过挑选，训练集中包含14973对轮廓和人脸照片，测试集中包含4992对轮廓和人脸照片。

以轮廓作为草图有很多好处，其中重要的一点是它比边缘图更加简洁，更符合我们手绘的习惯。因此用轮廓拟合手绘草图的数据分布更加合理。
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6 \textwidth]{contour_sample.png}
	\caption{轮廓图数据集示例}
	\label{fig:contour_sample}
\end{figure}

\subsection{手绘草图的生成}

为了测试模型对手绘草图的真实效果，我们开发了一个用于绘制和收集草图并能实时生成、展示结果的交互界面。我们在绘制窗口设置训练草图的平均脸为底图，作为手绘时的参照。设置笔触的大小为2像素值，从而保证与训练数据线条宽度相同。我们可以开放交互界面让不同的人来画，绘制的草图会自动存入我们的手绘草图数据库，从而可以不断丰富扩大我们的数据集。交互界面如图~\ref{fig:drawing_board}所示：
~\\
\begin{figure}[htb]
	\centering
	\includegraphics[width=1 \textwidth]{drawing_board.png}
	\caption{手绘草图交互界面示意图}
	\label{fig:drawing_board}
\end{figure}





