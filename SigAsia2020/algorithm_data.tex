% !TeX root = SketchFace.tex


Paired face sketch-photo dataset is required for supervised sketch-to-face translation methods.
Since there exits no large-scale paired face sketch dataset, the training face sketches used by existing methods are generated from face image dataset, e.g. CelebA-HQ face dataset, using edge detection algorithm such as HED~\cite{HED}.
%
However, the sketches generated by edge detection algorithm are sometimes incomplete or \td{other problems}. 
\td{Discuss the advance of make-edges over edge maps and contours}
\cxj{I would say the edge maps are quite different from handdrawn sketches, not because of the incompleteness.}
\rmv{ Although CSAGAN~\cite{CSAGAM} applied self-attention mechanism to alleviate the incompleteness problem, the others remains. Therefore we use another method to generate clearer and complete sketches from face images.}

~\cite{pix2pixHD} introduce another method to generate sketches from face images. Given a face image, the face landmarks are detected using an off-shelf landmark detection model. A new kind of sketch, denoted as \textit{face contour}, is obtained by connect specific landmarks. Sketch-to-face model trained by face contour fail to generalize to hand-drawn sketches with hair, wrinkles or beard. 

The CelebAMask-HQ dataset~\cite{CelebAMask-HQ} provides a face semantic map for each face image in CelebA-HQ dataset. We basically use the boundary map of the semantic map as the sketch of the corresponding face image. Figure~\ref{fig:sketch_data} shows an example of comparison between a sketch generated by edge detector, a face contour and a sketch generated from semantic boundary.
%


\paragraph{Stroke Deformation}
A shortcut of sketches generated from semantic boundary (and those generated by edge detector) is the lines of sketches are perfectly aligned to edges of the corresponding face images. In order to break the edge-alignment and mimic the hand-drawn sketches, we apply a deformation to the lines, similar to that in FaceShop~\cite{FaceShop}. Specifically, we vectorize lines of each sketches using AutoTrace algorithm~\cite{AutoTrace}, and add an offset randomly selected from $[-d, d]^2$ to the control points and end points of the vectorized lines, where $d$ is the maximum offset and we set $d=11$ in our experiments.
%
Both the initial sketches generated from semantic boundary and deformed sketches are used as the input sketches to our model.


\begin{figure}
	\centering
	\vspace{1.0cm}
	\caption{Comparison between a sketch generated from edge detection and from semantic boundary.}
	\label{fig:sketch_data}
\end{figure}