\section{Deep Network for Sketch-Photo Translation}
\label{sec:network}

\subsection{Edge alignment in baseline Model}

Generating a realistic photo from sketch can be considered as an image translation problem. 
We use the state-of-the-art Pix2PixHd~\cite{} as our baseline. 
% 
 
We use the CelebA-HD dataset~\cite{} which contains \td{xxx face images in WxH}. All the face images are globally aligned according to their eye positions. 
For each photo, we generate sketch samples \td{by xxx method}.
\td{XX for training and xx for testing.}
%
Using this paired sketch-photo dataset, \td{we trained our method and other state-of-the-art approaches for comparison}.

\subsubsection{Global alignment}
While all the face images in the CelebA dataset \cite{} are globally aligned with their eye positions, the learned generator implicitly embeds the global layout. Once the drawn sketches deviate from this implicitly embedded layout, the learned translation model generate awkward results, as Fig.~\ref{fig:global-align-fail} shows. 


\begin{figure}
	\centering
	\vspace{1.0cm}
	\caption{Generated face images from a sketch that does not follow the aligned face layout.}
	\label{fig:global-align-fail}
\end{figure}

\subsubsection{Data augmentation with geometric translation}
A straightforward way is to augment the training set by random geometric transformation of input sketches. 
However, large interval/range of the transform yields un-convergence of the network training. 
We limit the transformation range to $(-\frac{\pi}{10},\frac{\pi}{10})$ rotation, $(-\frac{W}{20},\frac{W}{20})$ translation, and \cxj{scale?} in order to increase the tolerance of the trained model on the distortion and roughness of hand-drawn sketches. 
%
Moreover, as an interactive system, we also simply provide a reference sketch, like ShadowDrawing~\cite{}.
We place an averaged face contour image on the canvas to provide a reference coordinate system for user to draw their strokes. 
Therefore, the drawn sketches under this geometric reference will be located in or close to space of the training sketches.

\subsubsection{Data augmentation with sketch dilation}

\subsubsection{pix2pixHD without instance normalization}
The baseline model, as well as many existing stylization DNNs, uses spatial normalization to extract style statistics, treating them as spatially uniform on the entire image. 
However, the shallow convolution layers typically extract texture or brightness statistics, which are varying dramastically on different local regions of a drand-drawn sketch. For example, there might be a large number strokes around hair regions or mouth regions to describe details. These heavy strokes bring significant difference while instance normalization at each convolution layer normalizes local patch features with a global factor. Therefore, the extracted features for identical eye strokes could be very different. 

We think that strokes mainly describe the shape features, without little texture information. 

\input{sap}





\paragraph{Sketch Data Generation}
Four options to generate sketches: Canny, HED, AutoTrace [Web..], and Contour. 

. 